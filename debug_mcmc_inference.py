"""
Debug script for BlockMCMC inference.
Enables all debug flags to trace the confidence tracking issue.

Usage:
    python debug_mcmc_inference.py
"""

import os
import torch
import time
import logging
from transformers import AutoTokenizer, AutoConfig
from vllm import distributed
from vllm.config import ParallelConfig
from vllm.config import VllmConfig, set_current_vllm_config

from dinfer.model import LLaDAMoeModelLM
from dinfer import BlockIteratorFactory, KVCacheFactory
from dinfer import MCMCThresholdParallelDecoder, BlockMCMCDiffusionLLM

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# å¯ç”¨è°ƒè¯•å¼€å…³
# ============================================================================
def enable_debug_flags():
    """Enable all debug flags in MCMC components"""
    from dinfer.decoding.parallel_strategy import MCMCThresholdParallelDecoder
    from dinfer.decoding.generate_uniform import (
        MCMCDiffusionIteration, 
        MCMCBlockRunner, 
        BlockMCMCDiffusionLLM
    )
    
    # å¯ç”¨è°ƒè¯•
    MCMCThresholdParallelDecoder.DEBUG_MCMC_DECODER = True
    MCMCDiffusionIteration.DEBUG_MCMC_ITERATION = True
    MCMCBlockRunner.DEBUG_MCMC_BLOCK_RUNNER = True
    BlockMCMCDiffusionLLM.DEBUG_MCMC_GENERATE = True
    
    print("âœ… All debug flags enabled")


def setup_distributed():
    """Initialize distributed environment"""
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12347'  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
    
    distributed.init_distributed_environment(1, 0, 'env://', 0, 'nccl')
    distributed.initialize_model_parallel(1, backend='nccl')


def main():
    print("=" * 60)
    print("Debug BlockMCMC Inference")
    print("=" * 60)
    
    # å¯ç”¨è°ƒè¯•
    enable_debug_flags()
    
    # è®¾ç½®
    model_path = "/home/zhounan/models/inclusionAI/LLaDA-MoE-7B-A1B-Instruct-fused"
    device = torch.device('cuda:0')
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    print("\nğŸ”§ Setting up distributed environment...")
    setup_distributed()
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    parallel_config = ParallelConfig(enable_expert_parallel=True)
    with set_current_vllm_config(VllmConfig(parallel_config=parallel_config)):
        model_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model = LLaDAMoeModelLM(config=model_config).eval()
        model.load_weights(model_path, torch_dtype=torch.bfloat16)
        model = model.to(device)
    
    # åˆ›å»ºè§£ç å™¨
    mask_id = 156895
    eos_id = 156892
    
    decoder = MCMCThresholdParallelDecoder(
        temperature=0.9,
        threshold=0.9,
        mask_id=mask_id,
        eos_id=eos_id
    )
    
    # åˆ›å»º DLLMï¼ˆä½¿ç”¨è¾ƒå°çš„å‚æ•°ä»¥ä¾¿è°ƒè¯•ï¼‰
    print("\nğŸ—ï¸ Creating BlockMCMCDiffusionLLM...")
    dllm = BlockMCMCDiffusionLLM(
        model=model,
        decoder=decoder,
        iterator_factory=BlockIteratorFactory(True),
        cache_factory=None,  # ä¸ä½¿ç”¨ KV cache ç®€åŒ–è°ƒè¯•
        enable_mcmc=True,
        n_mcmc_steps=1,  # åªåš 1 æ­¥ MCMC ä»¥ä¾¿è°ƒè¯•
        mcmc_alpha=4.0,
        mcmc_temperature=0.9,
        tokenizer=tokenizer,
        verbose=False  # å…³é—­ verboseï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„è°ƒè¯•è¾“å‡º
    )
    
    # å‡†å¤‡è¾“å…¥ï¼ˆä½¿ç”¨è¾ƒé•¿çš„ prompt æ¥æµ‹è¯•æ›´å¤šå—ï¼‰
    prompt = "The vending machine sells drinks for 80 cents each. However, it gives you a 20-cent refund for each empty bottle you return. James has 2 dollars (200 cents). Assuming he can buy a drink, drink it, and immediately return the bottle for the refund (and repeat), how many drinks can he drink in total?"
    messages = [{"role": "user", "content": prompt}]
    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    input_ids = tokenizer(formatted_prompt)['input_ids']
    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)
    
    print(f"\nğŸ“ Input prompt: {prompt[:50]}...")
    print(f"Input tokens: {input_ids.shape[1]}")
    
    # ç”Ÿæˆï¼ˆä½¿ç”¨è¾ƒå¤§çš„é•¿åº¦æ¥æµ‹è¯•å¤šä¸ªå—ï¼‰
    print("\nğŸš€ Generating (with debug output)...")
    print("=" * 60)
    
    gen_length = 256  # è¾ƒé•¿çš„ç”Ÿæˆé•¿åº¦ï¼Œæµ‹è¯•å¤šä¸ªå—
    block_length = 64  # æ ‡å‡†å—é•¿åº¦
    
    start_time = time.time()
    output = dllm.generate(input_ids, gen_length=gen_length, block_length=block_length)
    end_time = time.time()
    
    print("=" * 60)
    print("\nğŸ“Š Results:")
    print(f"Generation time: {end_time - start_time:.2f}s")
    print(f"Output shape: {output.shape}")
    print(f"Total forwards: {dllm.num_forwards}")
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(output[0, input_ids.shape[1]:], skip_special_tokens=True)
    print(f"\nğŸ“„ Generated text:\n{generated_text}")
    
    print("\nâœ… Done!")


def cleanup_distributed():
    """Clean up distributed environment"""
    import torch.distributed as dist
    if dist.is_initialized():
        dist.destroy_process_group()
        print("ğŸ§¹ Distributed process group destroyed")


if __name__ == '__main__':
    try:
        main()
    finally:
        cleanup_distributed()
