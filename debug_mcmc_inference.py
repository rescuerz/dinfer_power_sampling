"""
Debug script for BlockMCMC inference.
Enables all debug flags to trace the confidence tracking and KV Cache behavior.

================================================================================
å‚æ•°è¯´æ˜
================================================================================
- mcmc_alpha: ç›®æ ‡åˆ†å¸ƒçš„ power å‚æ•°ï¼Œç”¨äºè®¡ç®—ç½®ä¿¡åº¦ log p^Î±(x)ï¼Œå½±å“ MH æ¥å—ç‡
- proposal_alpha: æè®®åˆ†å¸ƒçš„ power å‚æ•°ï¼Œç”¨äº token é€‰æ‹©æ—¶çš„ logits scaling
  - proposal_alpha=1.0: æ ‡å‡†è§£ç ï¼ˆä¸ Phase 1 ç›¸åŒï¼‰
  - proposal_alpha>1.0: power-scaled è§£ç ï¼Œæè®®æ›´é›†ä¸­äºé«˜æ¦‚ç‡ token
- mcmc_temperature: æè®®åˆ†å¸ƒæ¸©åº¦ï¼ˆé»˜è®¤ 0.9ï¼‰
- use_shift: æ˜¯å¦ä½¿ç”¨ shift è§£ç ï¼ˆä»…åœ¨ enable_mcmc=False æ—¶ç”Ÿæ•ˆï¼‰

================================================================================
å¸¸ç”¨å‘½ä»¤ç¤ºä¾‹
================================================================================

# 1. åŸºæœ¬è°ƒè¯•ï¼ˆä¸ä½¿ç”¨ KV Cacheï¼‰
python debug_mcmc_inference.py

# 2. è°ƒè¯• KV Cacheï¼ˆprefix æ¨¡å¼ï¼‰
python debug_mcmc_inference.py --use_kv_cache --kv_cache_type prefix

# 3. è°ƒè¯• KV Cacheï¼ˆdual æ¨¡å¼ï¼‰
python debug_mcmc_inference.py --use_kv_cache --kv_cache_type dual

# 4. è°ƒè¯• MCMC æè®®ç”Ÿæˆçš„ KV Cache åŠ é€Ÿ
python debug_mcmc_inference.py --use_kv_cache --kv_cache_type dual --mcmc_use_kv_cache

# 5. è°ƒè¯• MCMC æè®®ç”Ÿæˆï¼ˆä¸ä½¿ç”¨ KV Cacheï¼Œå³ä½¿ä¸»è§£ç ä½¿ç”¨ï¼‰
python debug_mcmc_inference.py --use_kv_cache --kv_cache_type dual --no_mcmc_kv_cache

# 6. è°ƒè¯•å•æ­¥ MCMCï¼ˆæœ€å°åŒ–è°ƒè¯•ï¼‰
python debug_mcmc_inference.py --n_mcmc_steps 1 --gen_length 64 --block_length 32

# 7. è°ƒè¯•å¤šæ­¥ MCMC
python debug_mcmc_inference.py --n_mcmc_steps 5 --gen_length 128 --block_length 32

# 8. ç¦ç”¨ MCMC è°ƒè¯•ï¼ˆä»…è°ƒè¯•æ‰©æ•£è§£ç ï¼Œé€€åŒ–ä¸º BlockWiseDiffusionLLMï¼‰
python debug_mcmc_inference.py --disable_mcmc


# 10. è°ƒè¯• power-scaled æè®®åˆ†å¸ƒï¼ˆproposal_alpha=4.0ï¼‰
python debug_mcmc_inference.py --proposal_alpha 4.0 --n_mcmc_steps 2


# 12. å®Œæ•´è°ƒè¯•é…ç½®
python debug_mcmc_inference.py \\
    --use_kv_cache --kv_cache_type dual \\
    --mcmc_use_kv_cache \\
    --n_mcmc_steps 2 \\
    --mcmc_alpha 4.0 --proposal_alpha 1.0 \\
    --gen_length 128 --block_length 32

================================================================================
"""

import os
import torch
import time
import logging
import argparse
from transformers import AutoTokenizer, AutoConfig
from vllm import distributed
from vllm.config import ParallelConfig
from vllm.config import VllmConfig, set_current_vllm_config

from dinfer.model import LLaDAMoeModelLM
from dinfer import BlockIteratorFactory, KVCacheFactory
from dinfer import MCMCThresholdParallelDecoder, BlockMCMCDiffusionLLM

__version__ = "1.1.0"

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(
        description='Debug BlockMCMC Diffusion LLM Inference',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Version
    parser.add_argument('--version', action='store_true', help='Print version and configuration info')
    
    # Model settings
    parser.add_argument('--model_path', type=str, 
                        default="/home/zhounan/models/inclusionAI/LLaDA-MoE-7B-A1B-Instruct-fused",
                        help='Path to the model')
    parser.add_argument('--device', type=str, default='cuda:0', help='Device to use')
    
    # Generation settings
    parser.add_argument('--gen_length', type=int, default=256, help='Generation length')
    parser.add_argument('--block_length', type=int, default=64, help='Block length')
    parser.add_argument('--temperature', type=float, default=0.9, help='Sampling temperature')
    parser.add_argument('--threshold', type=float, default=0.9, help='Confidence threshold')
    
    # MCMC settings
    parser.add_argument('--enable_mcmc', action='store_true', default=True, help='Enable MCMC refinement')
    parser.add_argument('--disable_mcmc', action='store_true', help='Disable MCMC refinement')
    parser.add_argument('--n_mcmc_steps', type=int, default=1, help='Number of MCMC steps per block (default: 1 for debug)')
    parser.add_argument('--mcmc_alpha', type=float, default=4.0, help='MCMC alpha (power parameter for target distribution)')
    parser.add_argument('--mcmc_temperature', type=float, default=0.9, help='MCMC temperature (default: 0.9)')
    
    # KV Cache settings
    parser.add_argument('--use_kv_cache', action='store_true', help='Enable KV cache for main decoding')
    parser.add_argument('--kv_cache_type', type=str, default='dual', choices=['prefix', 'dual'],
                        help='KV cache type: prefix or dual')
    
    # MCMC KV Cache settings
    parser.add_argument('--mcmc_use_kv_cache', action='store_true', default=False,
                        help='Enable KV cache acceleration in MCMC proposal generation')
    parser.add_argument('--no_mcmc_kv_cache', action='store_true',
                        help='Disable KV cache in MCMC proposal generation')
    
    # Proposal alpha settings
    parser.add_argument('--proposal_alpha', type=float, default=4.0,
                        help='Power parameter for proposal distribution in MCMC (default: 1.0). '
                             '1.0 = standard decoding, >1.0 = power-scaled decoding.')
    
    # Shift decoding (only effective when enable_mcmc=False)
    parser.add_argument('--use_shift', action='store_true', default=False,
                        help='Use shift decoding (only effective when MCMC is disabled)')
    
    # Debug settings
    parser.add_argument('--disable_debug', action='store_true', help='Disable debug output')
    parser.add_argument('--prompt', type=str, default=None, help='Custom prompt')
    
    args = parser.parse_args()
    
    # Handle enable/disable mcmc
    if args.disable_mcmc:
        args.enable_mcmc = False
    
    # Handle MCMC KV cache settings
    if args.no_mcmc_kv_cache:
        args.mcmc_use_kv_cache = False
    elif args.use_kv_cache and not args.no_mcmc_kv_cache:
        if not args.mcmc_use_kv_cache:
            args.mcmc_use_kv_cache = True
    
    return args


def print_version_info():
    """Print version and configuration information"""
    print("=" * 60)
    print(f"Debug BlockMCMC Inference Script v{__version__}")
    print("=" * 60)
    print("\nğŸ“¦ Dependencies:")
    print(f"  - PyTorch: {torch.__version__}")
    print(f"  - CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - CUDA version: {torch.version.cuda}")
        print(f"  - GPU: {torch.cuda.get_device_name(0)}")
    
    print("\nğŸ”§ Debug flags available:")
    print("  - MCMCThresholdParallelDecoder.DEBUG_MCMC_DECODER")
    print("  - MCMCDiffusionIteration.DEBUG_MCMC_ITERATION")
    print("  - MCMCBlockRunner.DEBUG_MCMC_BLOCK_RUNNER")
    print("  - BlockMCMCDiffusionLLM.DEBUG_MCMC_GENERATE")
    
    print("\nğŸ“– For full help, run: python debug_mcmc_inference.py --help")
    print("=" * 60)


def enable_debug_flags():
    """Enable all debug flags in MCMC components"""
    from dinfer.decoding.parallel_strategy import MCMCThresholdParallelDecoder
    from dinfer.decoding.generate_uniform import (
        MCMCDiffusionIteration, 
        MCMCBlockRunner, 
        BlockMCMCDiffusionLLM
    )
    
    # å¯ç”¨è°ƒè¯•
    MCMCThresholdParallelDecoder.DEBUG_MCMC_DECODER = True
    MCMCDiffusionIteration.DEBUG_MCMC_ITERATION = True
    MCMCBlockRunner.DEBUG_MCMC_BLOCK_RUNNER = True
    BlockMCMCDiffusionLLM.DEBUG_MCMC_GENERATE = True
    
    print("âœ… All debug flags enabled")


def setup_distributed():
    """Initialize distributed environment"""
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12347'  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
    
    distributed.init_distributed_environment(1, 0, 'env://', 0, 'nccl')
    distributed.initialize_model_parallel(1, backend='nccl')


def cleanup_distributed():
    """Clean up distributed environment"""
    import torch.distributed as dist
    if dist.is_initialized():
        dist.destroy_process_group()
        print("ğŸ§¹ Distributed process group destroyed")


def main():
    args = parse_args()
    
    # Handle --version flag
    if args.version:
        print_version_info()
        return
    
    print("=" * 60)
    print(f"Debug BlockMCMC Inference v{__version__}")
    print("=" * 60)
    
    # å¯ç”¨è°ƒè¯•ï¼ˆé™¤éæ˜¾å¼ç¦ç”¨ï¼‰
    if not args.disable_debug:
        enable_debug_flags()
    
    # Print configuration
    print("\nğŸ“‹ Debug Configuration:")
    print(f"  Model: {args.model_path}")
    print(f"  Device: {args.device}")
    print(f"  Generation length: {args.gen_length}")
    print(f"  Block length: {args.block_length}")
    
    print(f"\nğŸ¯ MCMC Settings:")
    print(f"  MCMC enabled: {args.enable_mcmc}")
    if args.enable_mcmc:
        print(f"  MCMC steps: {args.n_mcmc_steps}")
        print(f"  MCMC alpha (target): {args.mcmc_alpha}")
        print(f"  MCMC temperature: {args.mcmc_temperature}")
        print(f"  Proposal alpha: {args.proposal_alpha}")
    else:
        print(f"  Use shift: {args.use_shift}")
    
    print(f"\nğŸ’¾ KV Cache Settings:")
    print(f"  Main KV cache: {args.use_kv_cache}")
    if args.use_kv_cache:
        print(f"  KV cache type: {args.kv_cache_type}")
    print(f"  MCMC KV cache: {args.mcmc_use_kv_cache}")
    
    # è®¾ç½®
    device = torch.device(args.device)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    print("\nğŸ”§ Setting up distributed environment...")
    setup_distributed()
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    
    parallel_config = ParallelConfig(enable_expert_parallel=True)
    with set_current_vllm_config(VllmConfig(parallel_config=parallel_config)):
        model_config = AutoConfig.from_pretrained(args.model_path, trust_remote_code=True)
        model = LLaDAMoeModelLM(config=model_config).eval()
        model.load_weights(args.model_path, torch_dtype=torch.bfloat16)
        model = model.to(device)
    
    # åˆ›å»ºè§£ç å™¨
    mask_id = tokenizer.convert_tokens_to_ids('[MASK]') if '[MASK]' in tokenizer.get_vocab() else 156895
    eos_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 156892
    
    print(f"Using mask_id={mask_id}, eos_id={eos_id}")
    
    decoder = MCMCThresholdParallelDecoder(
        temperature=args.temperature,
        threshold=args.threshold,
        mask_id=mask_id,
        eos_id=eos_id
    )
    
    # åˆ›å»º KV Cache å·¥å‚
    cache_factory = None
    if args.use_kv_cache:
        cache_factory = KVCacheFactory(args.kv_cache_type)
        print(f"Using KV cache type: {args.kv_cache_type}")
    
    # åˆ›å»º DLLM
    print("\nğŸ—ï¸ Creating BlockMCMCDiffusionLLM...")
    dllm = BlockMCMCDiffusionLLM(
        model=model,
        decoder=decoder,
        iterator_factory=BlockIteratorFactory(True),
        cache_factory=cache_factory,
        enable_mcmc=args.enable_mcmc,
        n_mcmc_steps=args.n_mcmc_steps,
        mcmc_alpha=args.mcmc_alpha,
        mcmc_temperature=args.mcmc_temperature,
        mcmc_use_kv_cache=args.mcmc_use_kv_cache,  # MCMC æè®®ç”Ÿæˆæ˜¯å¦ä½¿ç”¨ KV Cache
        proposal_alpha=args.proposal_alpha,  # æè®®åºåˆ—çš„ power scaling å‚æ•°
        use_shift=args.use_shift,  # æ˜¯å¦ä½¿ç”¨ shift è§£ç  (ä»…åœ¨ enable_mcmc=False æ—¶ç”Ÿæ•ˆ)
        tokenizer=tokenizer,
        verbose=False  # å…³é—­ verboseï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„è°ƒè¯•è¾“å‡º
    )
    
    # å‡†å¤‡è¾“å…¥
    if args.prompt is None:
        prompt = "The vending machine sells drinks for 80 cents each. However, it gives you a 20-cent refund for each empty bottle you return. James has 2 dollars (200 cents). Assuming he can buy a drink, drink it, and immediately return the bottle for the refund (and repeat), how many drinks can he drink in total?"
    else:
        prompt = args.prompt
    
    messages = [{"role": "user", "content": prompt}]
    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    input_ids = tokenizer(formatted_prompt)['input_ids']
    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)
    
    print(f"\nğŸ“ Input prompt: {prompt[:50]}...")
    print(f"Input tokens: {input_ids.shape[1]}")
    
    # ç”Ÿæˆ
    print("\nğŸš€ Generating (with debug output)...")
    print("=" * 60)
    
    start_time = time.time()
    output = dllm.generate(input_ids, gen_length=args.gen_length, block_length=args.block_length)
    end_time = time.time()
    
    print("=" * 60)
    print("\nğŸ“Š Results:")
    print(f"Generation time: {end_time - start_time:.2f}s")
    print(f"Output shape: {output.shape}")
    print(f"Total forwards: {dllm.num_forwards}")
    
    if args.enable_mcmc and dllm.proposal_generator is not None:
        print(f"  - Diffusion forwards: {dllm.diff_iteration.num_forwards}")
        print(f"  - Proposal forwards: {dllm.proposal_generator.num_forwards}")
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(output[0, input_ids.shape[1]:], skip_special_tokens=True)
    print(f"\nğŸ“„ Generated text:\n{generated_text}")
    
    # é¢å¤–è°ƒè¯•ä¿¡æ¯
    print("\nğŸ” Debug info:")
    generated_part = output[0, input_ids.shape[1]:]
    mask_count = (generated_part == mask_id).sum().item()
    eos_count = (generated_part == eos_id).sum().item()
    unique_tokens = torch.unique(generated_part).shape[0]
    
    print(f"  Mask tokens remaining: {mask_count}")
    print(f"  EOS tokens: {eos_count}")
    print(f"  Unique tokens: {unique_tokens}")
    
    print("\nâœ… Done!")


if __name__ == '__main__':
    try:
        main()
    finally:
        cleanup_distributed()